import torch
import numpy as np
import os
import cv2
from smplx import SMPL    
import argparse

from t3drender.render.render_functions import render_mp, MeshRenderer, SoftPhongShader, PointLights
from t3drender.cameras import PerspectiveCameras
from pytorch3d.structures import Meshes
from pytorch3d.renderer import TexturesVertex


def load_merged_camera_data(data_para_path):
    """åŠ è½½åˆå¹¶åçš„æ‘„åƒæœºå‚æ•°"""
    camera_data = {}
    
    for file in os.listdir(data_para_path):
        if file.startswith('cam') and file.endswith('.npz'):
            cam_name = file[:-4]
            cam_file = os.path.join(data_para_path, file)
            cam_params = dict(np.load(cam_file, allow_pickle=True))
            camera_data[cam_name] = cam_params
    
    return camera_data


def load_merged_person_data(data_para_path):
    """åŠ è½½åˆå¹¶åçš„äººä½“æ—¶åºæ•°æ®"""
    person_data = {}
    
    for file in os.listdir(data_para_path):
        if file.startswith('smpl_') and file.endswith('.npz'):
            person_name = file[5:-4]
            person_file = os.path.join(data_para_path, file)
            person_params = dict(np.load(person_file, allow_pickle=True))
            person_data[person_name] = person_params

    return person_data


def draw_kps(image, keypoints, skeleton, point_color=(0, 255, 0), line_color=(255, 0, 0), point_radius=2, line_thickness=2):
    """ç»˜åˆ¶2Då…³é”®ç‚¹"""
    if keypoints is None:
        return image
        
    keypoints = keypoints.copy()[:, :2].astype(np.int32)
    img = image.copy()
    
    # ç»˜åˆ¶éª¨æ¶è¿çº¿
    for connection in skeleton:
        start_idx, end_idx = connection
        if start_idx < len(keypoints) and end_idx < len(keypoints):
            start_point = tuple(keypoints[start_idx]) 
            end_point = tuple(keypoints[end_idx])      
            if np.all(start_point) and np.all(end_point):
                cv2.line(img, start_point, end_point, line_color, line_thickness)

    # ç»˜åˆ¶å…³é”®ç‚¹
    for i, (x, y) in enumerate(keypoints):
        if x > 0 and y > 0:
            cv2.circle(img, (x, y), point_radius, point_color, -1)

    return img


# COCOå…³é”®ç‚¹ç¼–å·å¯¹åº”è¡¨ï¼š
# 0:nose  1:left_eye  2:right_eye  3:left_ear  4:right_ear
# 5:left_shoulder  6:right_shoulder  7:left_elbow  8:right_elbow
# 9:left_wrist  10:right_wrist  11:left_hip  12:right_hip
# 13:left_knee  14:right_knee  15:left_ankle  16:right_ankle

# ç®€åŒ–çš„èº«ä½“ä¸»è¦å…³é”®ç‚¹éª¨æ¶è¿æ¥ï¼ˆåªä¿ç•™æ ¸å¿ƒå§¿æ€ï¼Œå»æ‰å¤´éƒ¨ç»†èŠ‚ï¼‰
BODY_SKELETON_SIMPLE = [
    # ä¸ŠåŠèº«ä¸»å¹²
    (5, 6),                   # å·¦è‚©(5)->å³è‚©(6)
    (5, 11), (6, 12),         # å·¦è‚©(5)->å·¦è‡€(11), å³è‚©(6)->å³è‡€(12)
    (11, 12),                 # å·¦è‡€(11)->å³è‡€(12)
    
    # å·¦è‡‚
    (5, 7), (7, 9),           # å·¦è‚©(5)->å·¦è‚˜(7)->å·¦æ‰‹è…•(9)
    
    # å³è‡‚  
    (6, 8), (8, 10),          # å³è‚©(6)->å³è‚˜(8)->å³æ‰‹è…•(10)
    
    # å·¦è…¿
    (11, 13), (13, 15),       # å·¦è‡€(11)->å·¦è†(13)->å·¦è„šè¸(15)
    
    # å³è…¿
    (12, 14), (14, 16),       # å³è‡€(12)->å³è†(14)->å³è„šè¸(16)
]


def draw_keypoints_on_image(image, keypoints_2d, person_name, downsample_factor=1, 
                          point_radius=2, line_thickness=2):
    """åœ¨å›¾åƒä¸Šç»˜åˆ¶2Då…³é”®ç‚¹"""
    if keypoints_2d is None:
        return image
    
    if downsample_factor > 1:
        keypoints_2d_scaled = keypoints_2d.copy()
        keypoints_2d_scaled[:, :2] /= downsample_factor
    else:
        keypoints_2d_scaled = keypoints_2d
    
    # ä¸ºä¸åŒäººç‰©ä½¿ç”¨ä¸åŒé¢œè‰²
    person_colors = {
        'aria01': ((0, 255, 0), (0, 200, 0)),
        'aria02': ((255, 0, 0), (200, 0, 0)),
        'aria03': ((0, 0, 255), (0, 0, 200)),
        'aria04': ((255, 255, 0), (200, 200, 0))
    }
    
    point_color, line_color = person_colors.get(person_name, ((0, 255, 0), (255, 0, 0)))
    
    return draw_kps(image, keypoints_2d_scaled, BODY_SKELETON_SIMPLE, 
                   point_color=point_color, line_color=line_color,
                   point_radius=point_radius, line_thickness=line_thickness)


def vis_smpl(smpl_verts, body_model, device, cameras, resolution=(512, 512), light_location=[0, 0, 0]):
    """æ¸²æŸ“SMPLæ¨¡å‹"""
    faces = torch.Tensor(body_model.faces.astype(np.int64)).to(device)[None].repeat_interleave(len(smpl_verts), dim=0)
    smpl_meshes = Meshes(verts=torch.Tensor(smpl_verts).to(device), faces=faces)
    color_tensor = torch.ones_like(smpl_meshes.verts_padded())
    color_tensor[..., :3] = torch.tensor([1, 1, 1]).to(device)[None][None]
    smpl_meshes.textures = TexturesVertex(color_tensor)
    
    image_tensors = render_rgb_with_lights(smpl_meshes, device=device, resolution=resolution, cameras=cameras, light_location=light_location)
    return (image_tensors.cpu().numpy() * 255).astype(np.uint8)


def vis_multiple_smpl(smpl_verts_list, body_model, device, cameras, resolution=(512, 512), colors=None, light_location=[0, 0, 0]):
    """æ¸²æŸ“å¤šä¸ªSMPLæ¨¡å‹"""
    if not smpl_verts_list:
        return None
    
    if colors is None:
        default_colors = [
            [1.0, 0.7, 0.7], [0.7, 1.0, 0.7], [0.7, 0.7, 1.0],
            [1.0, 1.0, 0.7], [1.0, 0.7, 1.0], [0.7, 1.0, 1.0],
        ]
        colors = [default_colors[i % len(default_colors)] for i in range(len(smpl_verts_list))]
    
    # åˆå¹¶æ‰€æœ‰é¡¶ç‚¹å’Œé¢
    all_verts = []
    all_faces = []
    all_colors = []
    
    vertex_offset = 0
    for i, smpl_verts in enumerate(smpl_verts_list):
        all_verts.append(torch.Tensor(smpl_verts).to(device))
        faces = torch.Tensor(body_model.faces.astype(np.int64)).to(device) + vertex_offset
        all_faces.append(faces)
        person_color = torch.ones((smpl_verts.shape[1], 3)).to(device) * torch.Tensor(colors[i]).to(device)
        all_colors.append(person_color)
        vertex_offset += smpl_verts.shape[1]
    
    combined_verts = torch.cat(all_verts, dim=1)
    combined_faces = torch.cat(all_faces, dim=0)
    combined_colors = torch.cat(all_colors, dim=0)
    
    batch_size_actual = combined_verts.shape[0]
    smpl_meshes = Meshes(
        verts=combined_verts, 
        faces=combined_faces[None].repeat_interleave(batch_size_actual, dim=0)
    )
    
    color_tensor = combined_colors[None].repeat_interleave(batch_size_actual, dim=0)
    smpl_meshes.textures = TexturesVertex(color_tensor)
    
    image_tensors = render_rgb_with_lights(smpl_meshes, device=device, resolution=resolution, cameras=cameras, light_location=light_location)
    return (image_tensors.cpu().numpy() * 255).astype(np.uint8)


def composite_mesh_on_rgb(rgb_image, rendered_mesh, alpha=0.7):
    """å°†æ¸²æŸ“çš„meshå åŠ åˆ°RGBå›¾åƒä¸Š"""
    if rgb_image.shape[:2] != rendered_mesh.shape[:2]:
        rendered_mesh = cv2.resize(rendered_mesh, (rgb_image.shape[1], rgb_image.shape[0]))
    
    if rendered_mesh.shape[2] == 4:
        alpha_channel = rendered_mesh[:, :, 3]
        rendered_mesh_rgb = rendered_mesh[:, :, :3]
        
        if alpha_channel.max() > 0:
            mesh_mask = (alpha_channel > 0.5).astype(np.float32)
        else:
            return rgb_image
    else:
        rendered_mesh_rgb = rendered_mesh
        white_background = np.all(rendered_mesh_rgb >= 250, axis=2)
        black_background = np.all(rendered_mesh_rgb <= 5, axis=2)
        mesh_mask = (~white_background & ~black_background).astype(np.float32)
        
        if mesh_mask.sum() == 0:
            color_std = np.std(rendered_mesh_rgb, axis=2)
            mesh_mask = (color_std > 10).astype(np.float32)
    
    if mesh_mask.sum() > 0:
        kernel = np.ones((3,3), np.uint8)
        mesh_mask = cv2.morphologyEx(mesh_mask, cv2.MORPH_OPEN, kernel)
        mesh_mask = cv2.morphologyEx(mesh_mask, cv2.MORPH_CLOSE, kernel)
        mesh_mask = cv2.GaussianBlur(mesh_mask, (3, 3), 0.5)
        mesh_mask = np.clip(mesh_mask, 0, 1)
    
    mesh_mask = mesh_mask[:, :, np.newaxis]
    
    composite = rgb_image.astype(np.float32) * (1 - mesh_mask * alpha) + \
                rendered_mesh_rgb.astype(np.float32) * (mesh_mask * alpha)
    
    return composite.astype(np.uint8)


def render_mesh_with_background(smpl_verts_list, body_model, device, cameras, rgb_image, 
                               colors=None, alpha=0.7, light_location=[0, 0, 0]):
    """æ¸²æŸ“meshå¹¶ä¸èƒŒæ™¯RGBå›¾åƒåˆæˆ"""
    H, W = rgb_image.shape[:2]
    
    if len(smpl_verts_list) == 1:
        rendered_mesh = vis_smpl(smpl_verts_list[0], body_model, device, cameras, 
                               resolution=(H, W), light_location=light_location)
    else:
        rendered_mesh = vis_multiple_smpl(smpl_verts_list, body_model, device, cameras,
                                        resolution=(H, W), colors=colors, light_location=light_location)
    
    if rendered_mesh is None:
        return rgb_image
    
    return composite_mesh_on_rgb(rgb_image, rendered_mesh[0], alpha=alpha)


def get_frame_data(person_data_dict, frame_idx):
    """ä»åˆå¹¶çš„æ—¶åºæ•°æ®ä¸­è·å–æŒ‡å®šå¸§çš„æ•°æ®"""
    frame_data = {}
    
    for person_name, person_data in person_data_dict.items():
        frame_indices = person_data['frame_indices']
        
        if frame_idx in frame_indices:
            time_idx = np.where(frame_indices == frame_idx)[0][0]
            
            smpl_params = person_data['smpl_params'].item()
            
            # æ£€æŸ¥SMPLå‚æ•°æ˜¯å¦ä¸ºç©ºï¼ˆå¦‚æœè¿™ä¸ªäººå‘˜æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼‰
            if (smpl_params['betas'].shape[0] == 0 or 
                smpl_params['body_pose'].shape[0] == 0 or
                smpl_params['global_orient'].shape[0] == 0 or
                smpl_params['transl'].shape[0] == 0):
                print(f"è·³è¿‡ {person_name}: SMPLå‚æ•°ä¸ºç©º")
                continue
            
            frame_smpl = {
                'betas': smpl_params['betas'][time_idx],
                'body_pose': smpl_params['body_pose'][time_idx],
                'global_orient': smpl_params['global_orient'][time_idx],
                'transl': smpl_params['transl'][time_idx]
            }
            
            poses3d = person_data['keypoints3d'][time_idx] if person_data['keypoints3d'][time_idx] is not None else None
            
            keypoints2d = {}
            keypoints2d_all = person_data['keypoints2d'].item()
            
            for cam_name in keypoints2d_all.keys():
                keypoints2d[cam_name] = keypoints2d_all[cam_name][time_idx] if keypoints2d_all[cam_name][time_idx] is not None else None

            frame_data[person_name] = {
                'smpl_params': frame_smpl,
                'poses3d': poses3d,
                'keypoints2d': keypoints2d
            }

    return frame_data


def render_rgb_with_lights(meshes, device, resolution=(512, 512), cameras=None, light_location=[0.0, 0.0, 0.0]):
    """å¸¦è‡ªå®šä¹‰å…‰æºçš„RGBæ¸²æŸ“å‡½æ•°"""
    mesh_renderer = MeshRenderer(resolution=resolution, shader=SoftPhongShader())

    if cameras is None:
        K = torch.eye(3, 3)[None]
        h, w = resolution
        K[:, 0, 0] = 512
        K[:, 1, 1] = 512
        K[:, 0, 2] = w / 2
        K[:, 1, 2] = h / 2
        cameras = PerspectiveCameras(in_ndc=False, K=K, convention='opencv', resolution=resolution)
    
    lights = PointLights(location=[light_location])
    rendered_frames = render_mp(renderer=mesh_renderer, meshes=meshes, lights=lights, cameras=cameras, device=device)
    return rendered_frames


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ä½¿ç”¨åˆå¹¶æ—¶åºæ•°æ®è¿›è¡ŒSMPLæ¸²æŸ“')
    parser.add_argument('--root', type=str, 
                       default="/gemini/user/private/3D/data/egohumans/01_tagging/004_tagging",
                       help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--downsample', type=int, default=2, help='é™é‡‡æ ·å› å­')
    parser.add_argument('--frame_step', type=int, default=1000, help='å¸§æ­¥é•¿')
    parser.add_argument('--mesh_alpha', type=float, default=1.0, help='meshé€æ˜åº¦')
    parser.add_argument('--light_position', type=str, default='0,0,0', help='å…‰æºä½ç½®')
    parser.add_argument('--keypoint_radius', type=int, default=2, help='å…³é”®ç‚¹åŠå¾„')
    parser.add_argument('--skeleton_thickness', type=int, default=2, help='éª¨æ¶çº¿æ¡ç²—ç»†')
    
    args = parser.parse_args()
    
    model_path = "/gemini/user/private/3D/data/body_models/smpl"
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    root = args.root
    data_para_path = f"{root}/data_para"
    
    downsample_factor = args.downsample
    frame_step = args.frame_step
    mesh_alpha = args.mesh_alpha
    light_position = [float(x) for x in args.light_position.split(',')]
    keypoint_radius = args.keypoint_radius
    skeleton_thickness = args.skeleton_thickness
    
    downsample_suffix = f"_ds{downsample_factor}" if downsample_factor > 1 else ""
    if frame_step > 1:
        downsample_suffix += f"_step{frame_step}"
    downsample_suffix += "_comp_kps"

    print(f"ğŸ¤– åˆå§‹åŒ–SMPLæ¨¡å‹: neutral ({model_path})")
    body_model = SMPL(model_path=model_path, gender="neutral", create_transl=False).to(device)
    
    print("ğŸ“· åŠ è½½æ‘„åƒæœºå‚æ•°...")
    camera_data = load_merged_camera_data(data_para_path)
    
    print("ğŸš¶ åŠ è½½äººä½“æ—¶åºæ•°æ®...")
    person_data_dict = load_merged_person_data(data_para_path)
    
    if person_data_dict:
        first_person = list(person_data_dict.keys())[0]
        frame_indices = person_data_dict[first_person]['frame_indices']
        num_frames = len(frame_indices)
        print(f"ğŸ“Š æ€»å¸§æ•°: {num_frames}")
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°äººä½“æ•°æ®")
        exit(1)
    
    output_dir = f"{root}/rendered_output{downsample_suffix}"
    os.makedirs(output_dir, exist_ok=True)
    
    selected_frames = frame_indices[::frame_step]
    print(f"ğŸ“Š å¤„ç†å¸§æ•°: {len(selected_frames)}/{len(frame_indices)}")
    
    for i, frame_idx in enumerate(selected_frames):
        if i % 10 == 0:
            print(f"   å¤„ç†å¸§ {frame_idx} ({i+1}/{len(selected_frames)})")
        
        frame_data = get_frame_data(person_data_dict, frame_idx)
        if not frame_data:
            continue
        
        all_smpl_vertices = []
        person_names = []
        
        for person_name, person_frame_data in frame_data.items():
            smpl_params = person_frame_data['smpl_params']
            
            betas = torch.Tensor(smpl_params["betas"]).to(device)[None]
            body_pose = torch.Tensor(smpl_params["body_pose"]).to(device)[None]
            global_orient = torch.Tensor(smpl_params["global_orient"]).to(device)[None]
            transl = torch.Tensor(smpl_params["transl"]).to(device)[None]
            
            smpl_output = body_model(betas=betas, body_pose=body_pose, global_orient=global_orient, transl=transl)
            all_smpl_vertices.append(smpl_output.vertices)
            person_names.append(person_name)
        
        for cam_name, cam_params in camera_data.items():
            K_orig = cam_params['K']
            R = cam_params['R']
            T = cam_params['T']
            
            frame_path = f"{root}/exo/{cam_name}/undistorted_images_scale2.0/{frame_idx:05d}.jpg"
            if not os.path.exists(frame_path):
                continue
                
            rgb_image = cv2.imread(frame_path)
            if rgb_image is None:
                continue
                
            H_orig, W_orig = rgb_image.shape[:2]
            H = H_orig // downsample_factor
            W = W_orig // downsample_factor
            H = H - (H % 2)
            W = W - (W % 2)
            
            if downsample_factor > 1:
                rgb_image = cv2.resize(rgb_image, (W, H))
            
            K = K_orig.copy()
            if downsample_factor > 1:
                K[..., 0, 0] /= downsample_factor
                K[..., 1, 1] /= downsample_factor
                K[..., 0, 2] /= downsample_factor
                K[..., 1, 2] /= downsample_factor
            
            render_camera = PerspectiveCameras(
                R=R, T=T, K=K, in_ndc=False, resolution=(H, W),
                device=device, convention="opencv"
            )
            
            # å•ç‹¬æ¸²æŸ“æ¯ä¸ªäººç‰©å¹¶åˆæˆ
            for person_name, smpl_vertices in zip(person_names, all_smpl_vertices):
                composite_image = render_mesh_with_background(
                    [smpl_vertices], body_model, device, render_camera, rgb_image,
                    alpha=mesh_alpha, light_location=light_position
                )
                
                person_frame_data = frame_data[person_name]
                keypoints_2d = person_frame_data['keypoints2d'].get(cam_name)
                if keypoints_2d is not None:
                    composite_image = draw_keypoints_on_image(
                        composite_image, keypoints_2d, person_name, 
                        downsample_factor=downsample_factor, 
                        point_radius=keypoint_radius, line_thickness=skeleton_thickness
                    )
                
                output_file = f"{output_dir}/{frame_idx:05d}_{person_name}_{cam_name}_comp.png"
                cv2.imwrite(output_file, composite_image)
            
            # æ¸²æŸ“æ‰€æœ‰äººç‰©åœ¨ä¸€èµ·å¹¶åˆæˆ
            if len(all_smpl_vertices) > 0:
                composite_image = render_mesh_with_background(
                    all_smpl_vertices, body_model, device, render_camera, rgb_image,
                    alpha=mesh_alpha, light_location=light_position
                )
                
                for person_name in person_names:
                    person_frame_data = frame_data[person_name]
                    keypoints_2d = person_frame_data['keypoints2d'].get(cam_name)
                    if keypoints_2d is not None:
                        composite_image = draw_keypoints_on_image(
                            composite_image, keypoints_2d, person_name, 
                            downsample_factor=downsample_factor, 
                            point_radius=keypoint_radius, line_thickness=skeleton_thickness
                        )
                
                person_names_str = "_".join(person_names)
                output_file = f"{output_dir}/{frame_idx:05d}_{person_names_str}_{cam_name}_comp_all.png"
                cv2.imwrite(output_file, composite_image)
    
    print(f"\nâœ… æ¸²æŸ“å®Œæˆ! è¾“å‡ºç›®å½•: {output_dir}") 