#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆçš„å•å¸§å¤„ç†è„šæœ¬
ä½¿ç”¨åˆå¹¶åçš„æ—¶åºæ•°æ®ï¼Œæ”¯æŒæŒ‡å®šå¸§å·å’Œçµæ´»çš„æ¸²æŸ“é€‰é¡¹
"""

import torch
import numpy as np
import os
import cv2
from smplx import SMPL    
import argparse
from t3drender.render.render_functions import render_rgb
from t3drender.cameras import PerspectiveCameras
from pytorch3d.structures import Meshes
from pytorch3d.renderer import TexturesVertex

import inspect
if not hasattr(inspect, 'getargspec'):
    inspect.getargspec = inspect.getfullargspec


def load_merged_data(data_para_path):
    """åŠ è½½åˆå¹¶åçš„æ‘„åƒæœºå’Œäººä½“æ•°æ®"""
    camera_data = {}
    person_data = {}
    
    # åŠ è½½æ‘„åƒæœºæ•°æ®
    for file in os.listdir(data_para_path):
        if file.startswith('cam') and file.endswith('.npz'):
            cam_name = file[:-4]
            cam_file = os.path.join(data_para_path, file)
            camera_data[cam_name] = dict(np.load(cam_file, allow_pickle=True))
    
    # åŠ è½½äººä½“æ•°æ®
    for file in os.listdir(data_para_path):
        if file.startswith('smpl_') and file.endswith('.npz'):
            person_name = file[5:-4]
            person_file = os.path.join(data_para_path, file)
            person_data[person_name] = dict(np.load(person_file, allow_pickle=True))
    
    return camera_data, person_data


def get_frame_data(person_data_dict, frame_idx):
    """ä»æ—¶åºæ•°æ®ä¸­æå–æŒ‡å®šå¸§çš„æ•°æ®"""
    frame_data = {}
    
    for person_name, person_data in person_data_dict.items():
        frame_indices = person_data['frame_indices']
        
        if frame_idx in frame_indices:
            time_idx = np.where(frame_indices == frame_idx)[0][0]
            
            # æå–SMPLå‚æ•°
            smpl_params = person_data['smpl_params'].item()
            frame_smpl = {
                'betas': smpl_params['betas'][time_idx],
                'body_pose': smpl_params['body_pose'][time_idx],
                'global_orient': smpl_params['global_orient'][time_idx],
                'transl': smpl_params['transl'][time_idx]
            }
            
            # æå–3Då…³é”®ç‚¹
            poses3d = person_data['poses3d'][time_idx] if len(person_data['poses3d']) > time_idx else None
            
            # æå–2Dæ•°æ®
            poses2d = {}
            bboxes2d = {}
            
            if 'poses2d' in person_data:
                poses2d_all = person_data['poses2d'].item()
                for cam_name in poses2d_all.keys():
                    poses2d[cam_name] = poses2d_all[cam_name][time_idx] if len(poses2d_all[cam_name]) > time_idx else None
            
            if 'bboxes2d' in person_data:
                bboxes2d_all = person_data['bboxes2d'].item()
                for cam_name in bboxes2d_all.keys():
                    bboxes2d[cam_name] = bboxes2d_all[cam_name][time_idx] if len(bboxes2d_all[cam_name]) > time_idx else None
            
            frame_data[person_name] = {
                'smpl_params': frame_smpl,
                'poses3d': poses3d,
                'poses2d': poses2d,
                'bboxes2d': bboxes2d
            }
    
    return frame_data


def vis_smpl(smpl_verts, body_model, device, cameras, batch_size=30, resolution=(512, 512), color=[1, 1, 1], verbose=False):
    """æ¸²æŸ“å•ä¸ªSMPLæ¨¡å‹"""
    smpl_meshes = Meshes(
        verts=torch.Tensor(smpl_verts).to(device), 
        faces=torch.Tensor(body_model.faces.astype(np.int64)).to(device)[None].repeat_interleave(len(smpl_verts), dim=0)
    )
    color_tensor = torch.ones_like(smpl_meshes.verts_padded())
    color_tensor[..., :3] = torch.Tensor(color).to(device)[None][None]
    smpl_meshes.textures = TexturesVertex(color_tensor)
    image_tensors = render_rgb(smpl_meshes, device=device, resolution=resolution, cameras=cameras, batch_size=batch_size, verbose=verbose)
    return (image_tensors.cpu().numpy() * 255).astype(np.uint8)


def vis_multiple_smpl(smpl_verts_list, body_model, device, cameras, batch_size=30, resolution=(512, 512), colors=None, verbose=False):
    """æ¸²æŸ“å¤šä¸ªSMPLæ¨¡å‹"""
    if not smpl_verts_list:
        return None
    
    # é»˜è®¤é¢œè‰²
    if colors is None:
        default_colors = [
            [1.0, 0.7, 0.7],  # Light red
            [0.7, 1.0, 0.7],  # Light green  
            [0.7, 0.7, 1.0],  # Light blue
            [1.0, 1.0, 0.7],  # Light yellow
            [1.0, 0.7, 1.0],  # Light magenta
            [0.7, 1.0, 1.0],  # Light cyan
        ]
        colors = [default_colors[i % len(default_colors)] for i in range(len(smpl_verts_list))]
    
    # åˆå¹¶æ‰€æœ‰é¡¶ç‚¹å’Œé¢
    all_verts = []
    all_faces = []
    all_colors = []
    
    vertex_offset = 0
    for i, smpl_verts in enumerate(smpl_verts_list):
        all_verts.append(torch.Tensor(smpl_verts).to(device))
        
        faces = torch.Tensor(body_model.faces.astype(np.int64)).to(device) + vertex_offset
        all_faces.append(faces)
        
        person_color = torch.ones((smpl_verts.shape[1], 3)).to(device) * torch.Tensor(colors[i]).to(device)
        all_colors.append(person_color)
        
        vertex_offset += smpl_verts.shape[1]
    
    # è¿æ¥æ‰€æœ‰æ•°æ®
    combined_verts = torch.cat(all_verts, dim=1)
    combined_faces = torch.cat(all_faces, dim=0)
    combined_colors = torch.cat(all_colors, dim=0)
    
    # åˆ›å»ºç½‘æ ¼
    batch_size_actual = combined_verts.shape[0]
    smpl_meshes = Meshes(
        verts=combined_verts, 
        faces=combined_faces[None].repeat_interleave(batch_size_actual, dim=0)
    )
    
    color_tensor = combined_colors[None].repeat_interleave(batch_size_actual, dim=0)
    smpl_meshes.textures = TexturesVertex(color_tensor)
    
    image_tensors = render_rgb(smpl_meshes, device=device, resolution=resolution, cameras=cameras, batch_size=batch_size, verbose=verbose)
    return (image_tensors.cpu().numpy() * 255).astype(np.uint8)


def draw_keypoints(image, keypoints, skeleton=None, color=(0, 255, 0), thickness=2, radius=3):
    """ç»˜åˆ¶2Då…³é”®ç‚¹"""
    if keypoints is None:
        return image
    
    img = image.copy()
    keypoints = np.array(keypoints)
    
    # ç»˜åˆ¶éª¨æ¶è¿æ¥
    if skeleton is not None:
        for connection in skeleton:
            start_idx, end_idx = connection
            if (start_idx < len(keypoints) and end_idx < len(keypoints) and 
                keypoints[start_idx][0] > 0 and keypoints[start_idx][1] > 0 and
                keypoints[end_idx][0] > 0 and keypoints[end_idx][1] > 0):
                pt1 = tuple(map(int, keypoints[start_idx][:2]))
                pt2 = tuple(map(int, keypoints[end_idx][:2]))
                cv2.line(img, pt1, pt2, color, thickness)
    
    # ç»˜åˆ¶å…³é”®ç‚¹
    for kp in keypoints:
        if len(kp) >= 2 and kp[0] > 0 and kp[1] > 0:
            center = tuple(map(int, kp[:2]))
            cv2.circle(img, center, radius, color, -1)
    
    return img


def process_frame(frame_idx, data_para_path, model_path="/gemini/user/private/3D/data/body_models/smpl", 
                  output_dir="./output", cameras=None, persons=None, options=None):
    """
    å¤„ç†æŒ‡å®šå¸§
    
    Args:
        frame_idx: å¸§ç´¢å¼•
        data_para_path: åˆå¹¶æ•°æ®ç›®å½•
        model_path: SMPLæ¨¡å‹è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        cameras: è¦å¤„ç†çš„æ‘„åƒæœºåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ‘„åƒæœº
        persons: è¦å¤„ç†çš„äººç‰©åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰äººç‰©
        options: æ¸²æŸ“é€‰é¡¹å­—å…¸
    """
    if options is None:
        options = {
            'render_individual': True,
            'render_all': True,
            'save_smpl': True,
            'save_2d': False,
            'device': 'cuda:0' if torch.cuda.is_available() else 'cpu'
        }
    
    device = torch.device(options['device'])
    print(f"ğŸ¬ å¤„ç†å¸§ {frame_idx} (è®¾å¤‡: {device})")
    
    # åŠ è½½SMPLæ¨¡å‹
    body_model = SMPL(
        model_path=model_path,
        gender="neutral",
        create_transl=False
    ).to(device)
    
    # åŠ è½½åˆå¹¶æ•°æ®
    camera_data, person_data = load_merged_data(data_para_path)
    
    # è¿‡æ»¤æ‘„åƒæœºå’Œäººç‰©
    if cameras is not None:
        camera_data = {k: v for k, v in camera_data.items() if k in cameras}
    if persons is not None:
        person_data = {k: v for k, v in person_data.items() if k in persons}
    
    print(f"ğŸ“· æ‘„åƒæœº: {list(camera_data.keys())}")
    print(f"ğŸš¶ äººç‰©: {list(person_data.keys())}")
    
    # è·å–è¯¥å¸§çš„æ•°æ®
    frame_data = get_frame_data(person_data, frame_idx)
    if not frame_data:
        print(f"âŒ å¸§ {frame_idx} æ²¡æœ‰æ•°æ®")
        return
    
    # è·å–å›¾åƒåˆ†è¾¨ç‡
    root_path = os.path.dirname(data_para_path)
    frame_path = f"{root_path}/exo/cam01/images/{frame_idx:05d}.jpg"
    if os.path.exists(frame_path):
        image = cv2.imread(frame_path)
        H, W = image.shape[:2]
    else:
        H, W = 512, 512
    
    print(f"ğŸ“ åˆ†è¾¨ç‡: {W}x{H}")
    
    # ç”ŸæˆSMPL vertices
    all_smpl_vertices = []
    person_names = []
    
    for person_name, person_frame_data in frame_data.items():
        smpl_params = person_frame_data['smpl_params']
        
        betas = torch.Tensor(smpl_params["betas"]).to(device)[None]
        body_pose = torch.Tensor(smpl_params["body_pose"]).to(device)[None]
        global_orient = torch.Tensor(smpl_params["global_orient"]).to(device)[None]
        transl = torch.Tensor(smpl_params["transl"]).to(device)[None]
        
        smpl_output = body_model(betas=betas, body_pose=body_pose, global_orient=global_orient, transl=transl)
        all_smpl_vertices.append(smpl_output.vertices)
        person_names.append(person_name)
    
    print(f"âœ… ç”Ÿæˆäº† {len(all_smpl_vertices)} ä¸ªSMPLæ¨¡å‹")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¸ºæ¯ä¸ªæ‘„åƒæœºæ¸²æŸ“
    for cam_name, cam_params in camera_data.items():
        print(f"ğŸ¥ æ¸²æŸ“æ‘„åƒæœº {cam_name}...")
        
        K = cam_params['K']
        R = cam_params['R'] 
        T = cam_params['T']
        
        render_camera = PerspectiveCameras(
            R=R,
            T=T,
            K=K,
            in_ndc=False,
            resolution=(H, W),
            device=device,
            convention="opencv"
        )
        
        # å•ç‹¬æ¸²æŸ“æ¯ä¸ªäººç‰©
        if options['render_individual'] and options['save_smpl']:
            for person_name, smpl_vertices in zip(person_names, all_smpl_vertices):
                rendered_image = vis_smpl(smpl_vertices, body_model, device, render_camera, 
                                        batch_size=30, resolution=(H, W), verbose=False)
                output_file = f"{output_dir}/{frame_idx:05d}_{person_name}_{cam_name}.png"
                cv2.imwrite(output_file, rendered_image[0, ..., :3])
                print(f"   ğŸ’¾ {output_file}")
        
        # æ¸²æŸ“æ‰€æœ‰äººç‰©åœ¨ä¸€èµ·
        if options['render_all'] and len(all_smpl_vertices) > 0 and options['save_smpl']:
            rendered_image = vis_multiple_smpl(all_smpl_vertices, body_model, device, render_camera, 
                                             batch_size=30, resolution=(H, W), verbose=False)
            person_names_str = "_".join(person_names)
            output_file = f"{output_dir}/{frame_idx:05d}_{person_names_str}_{cam_name}_all.png"
            cv2.imwrite(output_file, rendered_image[0, ..., :3])
            print(f"   ğŸ’¾ {output_file}")
        
        # ä¿å­˜2Då…³é”®ç‚¹å¯è§†åŒ–
        if options['save_2d'] and os.path.exists(frame_path):
            base_image = cv2.imread(frame_path)
            for person_name, person_frame_data in frame_data.items():
                poses2d = person_frame_data['poses2d']
                if cam_name in poses2d and poses2d[cam_name] is not None:
                    kp_image = draw_keypoints(base_image, poses2d[cam_name])
                    output_file = f"{output_dir}/{frame_idx:05d}_{person_name}_{cam_name}_2d.png"
                    cv2.imwrite(output_file, kp_image)
                    print(f"   ğŸ’¾ {output_file}")


def main():
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆå•å¸§å¤„ç†è„šæœ¬')
    parser.add_argument('--frame', type=int, required=True, help='è¦å¤„ç†çš„å¸§å·')
    parser.add_argument('--data_path', type=str, required=True, help='åˆå¹¶æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--model_path', type=str, default="/gemini/user/private/3D/data/body_models/smpl", help='SMPLæ¨¡å‹è·¯å¾„')
    parser.add_argument('--output', type=str, default="./output", help='è¾“å‡ºç›®å½•')
    parser.add_argument('--cameras', nargs='+', help='è¦å¤„ç†çš„æ‘„åƒæœºåˆ—è¡¨')
    parser.add_argument('--persons', nargs='+', help='è¦å¤„ç†çš„äººç‰©åˆ—è¡¨') 
    parser.add_argument('--no-individual', action='store_true', help='ä¸æ¸²æŸ“å•ä¸ªäººç‰©')
    parser.add_argument('--no-all', action='store_true', help='ä¸æ¸²æŸ“æ‰€æœ‰äººç‰©åˆå¹¶')
    parser.add_argument('--save-2d', action='store_true', help='ä¿å­˜2Då…³é”®ç‚¹å¯è§†åŒ–')
    parser.add_argument('--device', type=str, default='cuda:0', help='è®¡ç®—è®¾å¤‡')
    
    args = parser.parse_args()
    
    options = {
        'render_individual': not args.no_individual,
        'render_all': not args.no_all,
        'save_smpl': True,
        'save_2d': args.save_2d,
        'device': args.device
    }
    
    process_frame(
        frame_idx=args.frame,
        data_para_path=args.data_path,
        model_path=args.model_path,
        output_dir=args.output,
        cameras=args.cameras,
        persons=args.persons,
        options=options
    )
    
    print(f"âœ… å¸§ {args.frame} å¤„ç†å®Œæˆ!")


if __name__ == "__main__":
    main() 